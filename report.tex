\documentclass[titlepage,12pt,a4paper,times]{book}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{makeidx}
\usepackage{xspace}
\usepackage{graphicx,color,times}
\usepackage{fancyhdr}
% \usepackage{pxfonts}
% \usepackage{times}
% \usepackage{mathptm}
% \usepackage{amssymb}
% \usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage[printonlyused]{acronym}
\usepackage{float}
\usepackage{listings}
\usepackage{tocbibind}
\usepackage{wrapfig}
\usepackage[square]{natbib}
\usepackage{hyperref}
% \usepackage{glossaries}
% \makeglossaries
\usepackage{etoolbox}
\usepackage[section]{placeins}
\usepackage{enumitem}

% reset acronyms every chapter
\preto\chapter{\acresetall}

\renewcommand{\ttdefault}{phv}

\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancyhf{} \fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[LO]{\bfseries\rightmark}
\fancyhead[RE]{\bfseries\leftmark}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}
\addtolength{\headheight}{0.5pt}
\setlength{\marginparsep}{0cm}
\setlength{\marginparwidth}{0cm}
\setlength{\marginparpush}{0cm}
\addtolength{\hoffset}{-1.0cm}
\addtolength{\oddsidemargin}{\evensidemargin}
\addtolength{\oddsidemargin}{0.5cm}
\addtolength{\evensidemargin}{-0.5cm}


% NEW COLORS
\definecolor{dark}{gray}{0.25}
\definecolor{lgray}{gray}{0.9}
\definecolor{dkblue}{rgb}{0,0.13,0.4}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{ %
  language=C,                    basicstyle=\footnotesize,
  numbers=none,                  numberstyle=\tiny\color{gray},
  stepnumber=1,                  numbersep=5pt,
  backgroundcolor=\color{white}, showspaces=false,
  showstringspaces=false,        showtabs=false,
  frame=single,                  rulecolor=\color{black},
  tabsize=2,                     captionpos=b,
  breaklines=true,               breakatwhitespace=false,
  title=\lstname,                keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},  stringstyle=\color{mauve},
  escapeinside={\%*}{*)},        morekeywords={*},
  belowskip=0cm
}


\begin{document}

\include{cover}

\clearpage{\thispagestyle{empty}\cleardoublepage}

\frontmatter
\chapter*{Acknowledgments}
\label{chap:ack}

I would like to thank very many people for putting up with me and eventually I
will write this properly, but not right now :D

\tableofcontents

\clearpage{\thispagestyle{empty}\cleardoublepage}

\listoffigures

% Se n√£o existirem tabelas, comentar as seguintes linhas
\clearpage{\thispagestyle{empty}\cleardoublepage}

\listoftables
\clearpage{\thispagestyle{empty}\cleardoublepage}

% \listoflistings

\chapter*{Acronyms}
\begin{acronym}[SIFT]
	\acro{AGM}{Approximate Gaussian Mixture}
	\acro{AI}{Artificial Intelligence}
	\acro{BoW}{Bag of Words}
	\acro{CRF}{Conditional Random Field}
	\acro{DCT}{Discrete Cosine Transform}
	\acro{HOG}{Histogram of Oriented Gradient}
	\acro{KNN}{k-Nearest Neighbors}
	\acro{LSH}{Locality-Sensitive Hashing}
	\acro{SIFT}{Scale-Invariant Feature Transform}
	\acro{SVM}{Support Vector Machine}
\end{acronym}

% \clearpage{\pagestyle{empty}\cleardoublepage}
% \include{glossary}

\clearpage{\thispagestyle{empty}\cleardoublepage}

\mainmatter
\chapter{Introduction}
\label{chap:intro}
\nocite{*}
\section{Background}
\label{sec:amb}

The biometric recognition of individuals, has been acquiring research
interests, leading to the development of various systems, like the ones shown in
\citep{14, 15}.

It as also become relevant in the academics community the recognition in data
captured in non-controlled ambients. The main complication, with these
ambients, frequently associated with surveillance scenarios, is the low quality
of the data. In this kind of degraded data, is hardly possible to accurately
determine the identity of a person. However, is it possible to, at least,
extract some information, per example, the clothes used by the person.

\section{Motivation}
\label{sec:mot}

This specific area of biometric recognition has been growing importance
recently \citep{1, 4}. One of the reasons why that is happening, is because
clothing is an important contextual clue to identify a person \citep{2, 13}.

Another reason is the fast growing of the clothing sector in commerce
\citep{5}, so automatically providing statistics of the most used styles, can
be easily achieved, thought the analyses of surveillance videos.

This project can help accomplish the aforementioned by classifying clothes in
images captured in surveillance videos.

\section{Objectives}
\label{sec:obj}

In this work, it is intended to build a system capable of automatically
analysing the clothing of individuals and classifying it. In order to
accomplish this, it is required the study of different works done in the area,
the development of a system suited to classify clothing classes and conducting
various experiments and tests.

The study of other implementations is important, to help comprehend the main
issues with the development of such systems and also the learning of various
techniques and algorithms.

The implementation of the system requires a knowledge in multiple areas of
computer science, essentially \ac{AI}, Algorithms and Programming, and pattern
matching.

Lastly, conducting tests and experiments is highly relevant, beneficial to
adjusting parameters and understanding the flaws of the implementation, perhaps
adjusting them.

\section{Document organization}
\label{sec:organ}

This report is divided in five chapters, as described bellow:
\begin{enumerate}
	\item \textbf{\nameref{chap:intro}} -- characterizes the project, the
		motivation that led to its choice, its background and objectives.
	\item \textbf{\nameref{chap:ow}} -- presents other works somehow related
		with this project.
	\item \textbf{\nameref{chap:imp}} -- portrays the development of this
		project, demonstrating the steps to accomplish the objectives.
	\item \textbf{\nameref{chap:res}} -- shows the performed tests along with
		their results.
	\item \textbf{\nameref{chap:cfw}} -- concludes this report, presenting the
		main conclusions of this work, as well as what could have been
		implemented in order to improve the performance of the system.
\end{enumerate}

\chapter{Related Works}
\label{chap:ow}

\section{Introduction}
\label{chap2:sec:intro}

Clothing recognition has become a relevant and popular subject, so, many have
developed different systems, with different approaches in order to address it.
In this chapter, a few works done on the area are explored.

Section~\ref{chap2:sec:art1}, \nameref{chap2:sec:art1}, presentes an
analysis system capable of recognizing various clothing classes in surveillance
videos.

Section~\ref{chap2:sec:art2}, \nameref{chap2:sec:art2}, describes a system
suited to analyse the dressing style of a given collection of photos,
presenting the attributes existing in them.

Section~\ref{chap2:sec:art3}, \nameref{chap2:sec:art3}, particularizes an
adequate system for automatically suggesting clothing products based on an
input image.

\section{Real-Time Clothing Recognition in Surveillance Videos}
\label{chap2:sec:art1}

The video content analysis system, described in ~\citep{1}, is capable of
recognizing, in real-time, eight categories of clothing in multiple people.

As seen in the figure ~\ref{fig:vcasd} ~\citep{1}, first a face detection and
tracking are performed for each video frame. Afterwards, based on the face, it
is cropped a candidate rectangular region, containing the body of the person.
Once the face location and candidate region are known, occlusions are
determined. These are calculated according to every face location and
overlapped areas of the rectangles.

When people feature a visible frontal face and the non-occluded area is bigger
than 75\% it is possible to proceed with clothing segmentation. The candidate
rectangle region is segmented to roughly homogeneous color segments. It is then
applied the prior knowledge of foreground and background, based on face
alignment, in order to extract the foreground figure. Note that, to ensure the
applicability of the system it is not utilized motion information nor
background subtraction technique.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{images/Clothing_Diagram_1.png}
\caption{Video content analyses system diagram.}
\label{fig:vcasd}
\end{figure}
\FloatBarrier

A proper human figure alignment and clothing segmentation are required to
perform feature extraction and clothing classification. The results from
clothing segmentation might not be reliable for every frame. It is employed a
few cloth instances, with good segmentation quality, to calculate the average
feature vector to represent a cloth.

A cloth is represented by ten instances, including: gender and age estimation;
skin ratio of arms and legs; 2D color histograms; three texture descriptors.
\ac{HOG} is one of the used texture descriptors, to extract the features of
multiple spacial cells, drawn in figure ~\ref{fig:dtf}a ~\citep{1} as white
rectangles. The \ac{BoW}, another descriptor used, receives a bag of dense
\ac{SIFT} features, figure ~\ref{fig:dtf}b ~\citep{1}. The last texture
descriptor used is \ac{DCT}, figure ~\ref{fig:dtf}c ~\citep{1} In furtherance
of better results, clothes are analyzed in two sections, top and bottom, as
represented in figure ~\ref{fig:dtf} ~\citep{1}.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.4]{images/top_bottom.jpg}
\caption{Different textures features based on \ac{HOG}, \ac{BoW} and
\acs{DCT} responses.}
\label{fig:dtf}
\end{figure}
\FloatBarrier

All the previously mentioned instances of a cloth are concatenated as the
clothing representation. Each clothing category is learnt by a
one-against-all linear \ac{SVM}.

The precision rate of the presented system goes from 45.0\% up to 90.3\%,
in short-pants and short-skirts respectively, as shown in table ~\ref{tab:prds}
 ~\citep{1}.

 \begin{table}[!h]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Category} & \textbf{Precision}\\
\hline
\hline
\textbf{Suit (top)} & 87.5\% \\
\hline
\textbf{Suit (bottom)} & 85.7\% \\
\hline
\textbf{Shirt} & 81.8\% \\
\hline
\textbf{T-shirt} & 70.7\% \\
\hline
\textbf{Jeans} & 90.1\% \\
\hline
\textbf{Short pant} & 45.0\% \\
\hline
\textbf{Short skirt} & 90.3\% \\
\hline
\textbf{Long skirt} & 74.7\% \\
\hline
\end{tabular}
\caption{Precision rate of the described system.}
\label{tab:prds}
\end{table}
\FloatBarrier

\section{Describing Clothing by Semantic Attributes}
\label{chap2:sec:art2}

The main focus of the system, described in \citep{2}, is dressing style
analysis. It is capable of, given a collection of photos, analysing the
dressing style, therefore, make shopping recommendations. An example is
represented in figure ~\ref{fig:ids}~\citep{2}. This is a fully automatic
system that learns attributes for clothing on the human upper body.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.7]{images/2_3_fig0.jpg}
\caption{Inferred dressing style of a person. The wrong predictions are
highlighted in red.}
\label{fig:ids}
\end{figure}
\FloatBarrier

In figure ~\ref{fig:fc}~\citep{2} is illustrated the flowchart of the presented
system. Since estimating full body pose is still a challenging problem, after
all, the lower body is not always visible or is sometimes occluded, so the
system only analyses the upper body. Given an input image, human pose is
estimated, finding the upper torso and arms location, as seen in the example
shown in figure ~\ref{fig:eehp}~\citep{2}. The upper body is detected using a
complementary upper body detector and a face detector. The person is then
segmented from the background using the GrabCut~\citep{7} algorithm.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{images/2_3_fig2.jpg}
\caption{Flowchart of the system.}
\label{fig:fc}
\end{figure}
\FloatBarrier

\begin{figure}[!h]
\centering
\includegraphics[scale=0.9]{images/2_3_fig1.jpg}
\caption{Example of the estimation of a human pose.}
\label{fig:eehp}
\end{figure}
\FloatBarrier

Following, forty features are extracted and quantized. Since the diversity of
clothing attributes desired to learn is wide, a single type of features is
not likely to be accurate on all of them. Accordingly, for each attribute, the
classification of several features are gathered to obtain a prediction. There
are four types of base features, extracted from: \ac{SIFT}; texture descriptors
from Maximum Response filters; color in LAB space; skin probabilities from skin
detector. The system takes advantage of the recent improvements in human pose
estimation, by adaptively extracting image features from different human body
parts. Therefore, the sampling location, scale and orientation of the \ac{SIFT}
descriptors depend on the estimation of the human pose. In figure ~\ref{fig:est}
~\citep{2}, it is represented the extraction of the \ac{SIFT} over the torso of
a person. The system, lastly, extracts one more feature, called skin-excluded
color feature. The skin area, generated by the skin detector, is masked out
to extract the skin-excluded color feature.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.7]{images/2_3_fig3.jpg}
\caption{Extraction of the \ac{SIFT} over the torso region.}
\label{fig:est}
\end{figure}
\FloatBarrier

For each attribute, a \ac{SVM} classification is performed, using the
extracted traits. Each classifier produces a probability score.
Since clothing characteristics are correlated, mutual dependencies are also
explored between attributes. These dependencies then lead to the rules of style.
To the extent of modeling these rules, a \ac{CRF} is employed on top of the
classification provided by the individual classifiers. The inference result of
the \ac{CRF} produces the final list of attributes. Providing the probability
scores from the attribute classifier to the \ac{CRF}, leads to better results
than simply using a attribute classifier.

The precision rate of the presented system goes from 52.56\% up to 86.17\%,
in clothing attributes, as shown in table ~\ref{tab:ropsn}~\citep{2}. As for
clothing categories, rates go from 43.56\% to 80.65\%, ~\ref{tab:ropcc}
~\citep{2}.

\begin{table}[!h]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribute} & \textbf{Accuracy}\\
\hline
\hline
\textbf{No sleeve} & 86.17\% \\
\hline
\textbf{Short sleeve} & 64.90\% \\
\hline
\textbf{Long sleeve} & 84.04\% \\
\hline
\end{tabular}
\quad
\begin{tabular}{|l|r|}
\hline
\textbf{Attribute} & \textbf{Accuracy}\\
\hline
\hline
\textbf{V-shape} & 67.27\% \\
\hline
\textbf{Round} & 52.56\% \\
\hline
\textbf{Other style} & 54.71\% \\
\hline
\end{tabular}
\caption{Rate of predictions referring to sleeve length (left table) and
neckline shape (right table).}
\label{tab:ropsn}
\end{table}
\FloatBarrier

\begin{table}[!h]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Clothing} & \textbf{Accuracy}\\
\hline
\hline
\textbf{Shirt} & 43.56\% \\
\hline
\textbf{Sweater} & 54.84\% \\
\hline
\textbf{T-shirt} & 80.65\% \\
\hline
\textbf{Outerwear} & 61.30\% \\
\hline
\textbf{Suit} & 66.13\% \\
\hline
\textbf{Tank top} & 79.04\% \\
\hline
\textbf{Dress} & 56.45\% \\
\hline
\end{tabular}
\caption{Rate of predictions referring to clothing categories.}
\label{tab:ropcc}
\end{table}
\FloatBarrier

\section{Getting the Look: Clothing Recognition and Segmentation for Automatic
Products Suggestions in Everyday Photos}
\label{chap2:sec:art3}
\citep{3} presents a system capable of automatically suggest relevant clothing
products, when provided with a single image. Figure~\ref{fig:qisci}~\citep{3}
shows a real world image, the query image, on the left, and the clothing product
suggestions, on the right.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.6]{images/2_4_fig1.jpg}
\caption{The query image and the suggested clothing items.}
\label{fig:qisci}
\end{figure}
\FloatBarrier

The direction chosen, in order to achieve the presented results, is divided in
two main courses: detecting the clothing classes represented in a given image
and obtaining visually alike products.

For the clothing classification, first is performed a pose estimation,
based on the pose estimation algorithm of~\citep{8}, explained in detail
in Section~\ref{chap3:sec:bps}. Once this step is complete, the main focus
turns to the image regions that are most likely to contain clothes.

Using the estimated pose areas, a prior probability map of clothing appearance
is calculated and then normalized on those regions. Given a set of classes,
firstly, a prior probability map is created for each clothing class.
Afterwards, the maps of all classes are concatenated and normalized,
creating a global \emph{clothing prior probability map}.
This probability map, subsequently, is used as a binary mask, so that only
probable human regions for clothing are used. In figure~\ref{fig:ppm}~\citep{3},
the probability maps for belt, feet, torso and hip regions (left) and
the global \emph{clothing prior probability map} (right).

\begin{figure}[!h]
\centering
\includegraphics[scale=0.45]{images/2_4_fig2.jpg}
\caption{Prior probability maps, quantized and normalized, on estimated body
parts.}
\label{fig:ppm}
\end{figure}
\FloatBarrier

Once the aforesaid mask is calculated, it is possible to proceed to the
segmentation and clustering. In this phase, applying the binary mask, on the
detected body parts, is the primary step. This produces a image that only has
color in the regions of interest, as shown in the left image of figure
~\ref{fig:scs}~\citep{3}. Following, segmentation of \cite{9} is used, an
example of the results produced by it is presented in the middle image of
figure~\ref{fig:scs}~\citep{3}. Afterwards, to merge non-neighboring segments,
with the same visual appearance, a clustering step is performed on the extracted
segments. For the mentioned process, the adopted approach was the \ac{AGM}
algorithm~\citep{10}. In the right image of figure~\ref{fig:scs}~\citep{3}, are
the results of the merge between the clustering and the initial segments.


\begin{figure}[!h]
\centering
\includegraphics[scale=0.4]{images/2_4_fig3.jpg}
\caption{Segmentation and clustering steps.}
\label{fig:scs}
\end{figure}
\FloatBarrier

The next step consists on detecting if the segments correspond to a clothing
class and classify them. To accomplish this, four intuitions are considered: a
clothing class is always found in a consistent spacial region; clothing classes
are not distinguished solo by color and texture; shape alone is not enough
either, since there are highly inconsistent boundaries of clothing classes,
due to various human poses and views; representations must facilitate rapid
classification, therefore binary vectors are chosen to represent the segments.

Regarding the training set images, the exact clothing segment regions are given,
as well as their class label, making the process of segmentation and clustering
avoidable. Only the extraction of binary vectors is performed to each segment.
Matching each query vector with all training vectors is a poor choice in terms
of scalability. Accordingly, a sublinear algorithm is used to obtain the
most promising candidates. Once the binary vectors are extracted,
from each clothing segment of the training set, all the vectors of all segments
of all classes are added in a multi-probe \ac{LSH} index~\citep{11}, along
with the corresponding labels. Given a query vector, the \ac{LSH} index
returns the \emph{n} nearest neighbors of the query from the database, with
the corresponding classes. Subsequently, the probabilities, for each segment, are
calculated for each class, if it is higher than 0.5 the class is assigned to
that segment. Examples of this classification can be seen in figure
~\ref{fig:sccr}~\citep{3}.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.7]{images/2_4_fig4.jpg}
\caption{Sample of clothing classification results.}
\label{fig:sccr}
\end{figure}
\FloatBarrier

The next step consists in finding similar clothing suggestions from a large
product database. To segment the region of interest of the product in the
database, the GrabCut algorithm is used \citep{7}. Then to describe the visual
features of a region, color and texture characteristics are extracted.  After
having a characteristic vector for each image of the product collection, it is
necessary to retrieve the vectors that are similar to the query region vector.
The query vectors are classified, so it is only necessary to search for
familiar product within that class. To accomplish this, it is used a fast
approximate \ac{KNN} index, using a forest of \emph{randomized kd-trees}
~\citep{12}. For each search, the \emph{k} most similar products are selected
and then threshold the distance, so that only the closest results to the query
are displayed. If no results are below the distance threshold, that query
region is simple ignored.

\begin{table}[!h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Clothing Category} & \textbf{Relevant} & \textbf{Random}\\
\hline
\hline
\textbf{Dress} & 68\% & 10\% \\
\hline
\textbf{Skirt} & 59\% & 2\% \\
\hline
\textbf{Blouse} & 37\% & 4\% \\
\hline
\textbf{Top} & 55\% & 6\% \\
\hline
\textbf{Jackets \& Coats} & 43\% & 3\% \\
\hline
\textbf{Pants \& Jeans} & 69\% & 12\% \\
\hline
\textbf{Boots} & 66\% & 14\% \\
\hline
\hline
\textbf{All} & 54\% & 8\% \\
\hline
\end{tabular}
\caption{Evaluations of the suggested products.}
\label{tab:uesp}
\end{table}
\FloatBarrier

In order to test the developed system in large scale, a web application was
created. Provided with a real-world image, the application detects the clothing
classes existent, suggesting alike clothing products, for each class. Each
suggested product was evaluated as relevant, not relevant or random. The
results of the performed test are shown in table~\ref{tab:uesp}~\citep{3}.
Overall, the system suggested accurately 54\% of the products and had an
average of 8\% random suggestions.

% \section{High-Level Clothes Description Based on Colour-Texture and Structural
% Features}
% \label{chap2:sec:art4}

% \section{Mobile Visual Clothing Search}
% \label{chap2:sec:art5}

\section{Conclusions}
\label{chap2:sec:concs}

As seen through this chapter, clothing recognition is not an easy task.
Recognizing patterns and textures in order to classify clothes requires the
extraction of various features.

Section~\ref{chap2:sec:art2} and \ref{chap2:sec:art3} present systems meant to
work with higher definition images than the one described in section
~\ref{chap2:sec:art1}. Therefore, the first two mentioned, have a wider set of
characteristics, leading to a bigger amount of clothing classes. In
section~\ref{chap2:sec:art1}, is described a system designed to work in a very
similar ambient to the one presented in this report.

\chapter{Implementation}
\label{chap:imp}

\section{Introduction}
\label{chap3:sec:intro}

In order to create a system capable of classifying clothes, there are various
steps that must be followed. This chapter explains these steps, one by one.

It is important to mention, that the explained process is done to frame images
from videos of a surveillance camera, therefore the quality is very low.

Firstly given a video, from a database of surveillance videos, it is necessary
to extract the background, so that only the person is left. This is explained
in detail in section~\ref{chap3:sec:bs}~\nameref{chap3:sec:bs}.

Afterwards, as explained in section~\ref{chap3:sec:bps}, a
\nameref{chap3:sec:bps}~\citep{8} is performed.

In section~\ref{chap3:sec:cte}~\nameref{chap3:sec:cte}, is described the
process of features extraction.

Lastly, section~\ref{chap3:sec:ann}~\nameref{chap3:sec:ann}, explains the
process of classification of the clothes.

\section{Background Subtraction}
\label{chap3:sec:bs}

Background subtraction is important to reduce the error in the next step,
\nameref{chap3:sec:bps}. Since the query images have a very low resolution, it
is easy for the pose estimator to mistaken background objects with a person.
The background must be removed, so that only the region of interest is provided
to the pose estimator.

To accomplish the aforementioned, the binary mask, containing the information
of the foreground, figure~\ref{fig:bic}a, is multiplied with the original
image, figure~\ref{fig:bic}b, resulting in a image without background,
figure~\ref{fig:bic}c.

\begin{figure}[!h]
\centering
\includegraphics[scale=1]{images/3_2_fig1.jpg}
\caption{In the left, the binary mask, containing background information. In the
middle, the query image. In the right, the image after background removal.}
\label{fig:bic}
\end{figure}
\FloatBarrier

The image containing the background location, is previously provided as seen in
figure~\ref{fig:omnm}a. From that image, all the values higher than 129, seen
in figure~\ref{fig:omnm}a with gray color, are changed into 0 (black), then the
colors are inverted, resulting in the binary mask, seen in~\ref{fig:omnm}b,
used to multiply with the original image.

\begin{figure}[!h]
\centering
\includegraphics[scale=1]{images/3_2_fig2.jpg}
\caption{Original provided mask (left), binary mask with region of interest
represented as black (right).}
\label{fig:omnm}
\end{figure}
\FloatBarrier

\section{Body Pose Estimation}
\label{chap3:sec:bps}

From this step forward, the background has already been removed, so only
the region of interest is left to work with.

\cite{16} implemented the algorithm, described by \cite{8}, in
MATLAB\textsuperscript{\textregistered}, that is used to perform the pose
estimation in this work.  The pose estimator receives a query image and, if the
person is successfully detected, returns a set of 26 \emph{body parts},
represented as $P=\{p_1,\hdots,p_{26}\}$. Each $p_i$ portrays a square region
in the image, as seen in figure~\ref{fig:erpd}, with each color corresponding
to a body part.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.63]{images/3_3_fig1.jpg}
\caption{Example results of the pose detector from \cite{16}}
\label{fig:erpd}
\end{figure}
\FloatBarrier

Since the image resolution is too small, the squares do not allow a good
perception of what the area of each body part is, small modifications on the
code, permit a better view of those. In figure~\ref{fig:erpdm}, are presented
some examples of the altered code. Each dot ($*$) represents the middle of the
squares previously shown in figure~\ref{fig:erpd}. Now, its easily noticeable
that, each point is located on the corresponding body part.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.63]{images/3_3_fig2.jpg}
\caption{Example results of the pose detector after modifications}
\label{fig:erpdm}
\end{figure}
\FloatBarrier

However further alterations are required, the area of each body part is
necessary to later extract the features.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.63]{images/3_3_fig3_first.jpg}
\caption{Example results of the pose detector after modifications}
\label{fig:erpdm}
\end{figure}
\FloatBarrier

\begin{figure}[!h]
\centering
\includegraphics[scale=0.63]{images/3_3_fig4_final.jpg}
\caption{Example results of the pose detector after modifications}
\label{fig:erpdm}
\end{figure}
\FloatBarrier

\section{Color and Texture Extraction}
\label{chap3:sec:cte}

\section{Classification}
\label{chap3:sec:ann}

\section{Conclusions}
\label{chap3:sec:concs}

\chapter{Tests and Results}
\label{chap:res}

\section{Introduction}
\label{chap4:sec:intro}

\section{}
\label{chap4:sec:...}

\section{Conclusions}
\label{chap4:sec:concs}

\chapter{Conclusions and Future Work}
\label{chap:cfw}

\section{Main Conclusions}
\label{sec:main-conc}

Esta sec√ß√£o cont√©m a resposta √† quest√£o: \\
\emph{Quais foram as conclus√µes princ√≠pais a que o(a) aluno(a) chegou no fim
deste trabalho?}

\section{Future Work}
\label{sec:future-work}

Esta sec√ß√£o responde a quest√µes como:\\
\emph{O que √© que ficou por fazer, e porque?}\\
\emph{O que √© que seria interessante fazer, mas n√£o foi feito por n√£o ser
exatamente o objetivo deste trabalho?}\\
\emph{Em que outros casos ou situa√ß√µes ou cen√°rios -- que n√£o foram estudados
no contexto deste projeto por n√£o ser seu objetivo -- √© que o trabalho aqui
descrito pode ter aplica√ß√µes interessantes e porque?}

% SE EXISTIREM APENDICES, DESCOMENTAR O QUE EST√Å EM BAIXO
% \appendix
% \include{apendice1}
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \include{continuacao}
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \include{apendice2}
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \include{apendice3}
% \clearpage{\pagestyle{empty}\cleardoublepage}

\backmatter

\bibliographystyle{apalike-url}
\bibliography{bibliography}

\end{document}
